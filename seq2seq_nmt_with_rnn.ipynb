{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNLGZS6Ii_YE"
      },
      "source": [
        "# Seq2seq NMT with RNN\n",
        "\n",
        "\n",
        "\n",
        "[Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473)\n",
        "\n",
        "**NOTE:**\n",
        "\n",
        "-  use clean bpe data\n",
        "-  use a piece of triaing data during coding or low in credits\n",
        "\n",
        "You have to implement:\n",
        "\n",
        "- Encoder\n",
        "- Attention (Bahdanau)\n",
        "- Decoder\n",
        "\n",
        "Goal:\n",
        "\n",
        "- Loss in training, validation and test\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qxTE-EePi_YH"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dKtvwA_Rji-B"
      },
      "outputs": [],
      "source": [
        "#if you dont have bpe data use sacremoses tokenizer\n",
        "#!pip install sacremoses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jXnagBYmi_YI"
      },
      "outputs": [],
      "source": [
        "SEED = 42\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cT0G7DOArBFK",
        "outputId": "a22c6043-e73e-4a2f-939d-af29ce2f731a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gjVU2ynri_YJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52ee188b-5513-4beb-b309-2e645210fa49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchtext/data/__init__.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
            "/usr/local/lib/python3.10/dist-packages/torchtext/vocab/__init__.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
            "/usr/local/lib/python3.10/dist-packages/torchtext/utils.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
          ]
        }
      ],
      "source": [
        "import torchtext\n",
        "import torch\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from collections import Counter\n",
        "from torchtext.vocab import vocab\n",
        "from torchtext.utils import download_from_url, extract_archive\n",
        "import io\n",
        "\n",
        "#ran out of RAM on my other google account :/\n",
        "#poliaeva.polina@gmail.com and upb203040@gmail.com are both me\n",
        "#more accounts may be coming in the future with how quickly RAM gets used up :'D\n",
        "path = \"\"\n",
        "#0=en 1=de\n",
        "# soure and target data\n",
        "#NOTE: USE clean bpe data!\n",
        "train_filepaths = [path+'train.en-de.bpe.en', path+'train.en-de.bpe.de']\n",
        "val_filepaths = [path+'dev.en-de.bpe.en', path+'dev.en-de.bpe.de']\n",
        "test_filepaths = [path+'test.en-de.bpe.en', path+'test.en-de.bpe.de']\n",
        "\n",
        "\n",
        "#de_tokenizer = get_tokenizer('moses', language='de')\n",
        "#en_tokenizer = get_tokenizer('moses', language='en')\n",
        "\n",
        "\n",
        "def build_vocab(filepath, tokenizer=None):\n",
        "  counter = Counter()\n",
        "  with io.open(filepath, encoding=\"utf8\") as f:\n",
        "    for string_ in f:\n",
        "      #counter.update(tokenizer(string_))\n",
        "      counter.update(string_.split())\n",
        "  return vocab(counter, specials=['<unk>', '<pad>', '<bos>', '<eos>'])\n",
        "\n",
        "#Vocab\n",
        "en_vocab = build_vocab(train_filepaths[0])\n",
        "de_vocab = build_vocab(train_filepaths[1])\n",
        "\n",
        "en_vocab.set_default_index(en_vocab['<unk>'])\n",
        "de_vocab.set_default_index(de_vocab['<unk>'])\n",
        "\n",
        "def data_process(filepaths):\n",
        "  raw_en_iter = iter(io.open(filepaths[0], encoding=\"utf8\"))\n",
        "  raw_de_iter = iter(io.open(filepaths[1], encoding=\"utf8\"))\n",
        "  data = []\n",
        "  for (raw_en, raw_de) in zip(raw_en_iter, raw_de_iter):\n",
        "    en_tensor_ = torch.tensor([en_vocab[token] for token in raw_en.split()], #en_tokenizer(raw_en)\n",
        "                            dtype=torch.long)\n",
        "    de_tensor_ = torch.tensor([de_vocab[token] for token in raw_de.split()], #de_tokenizer(raw_de)\n",
        "                            dtype=torch.long)\n",
        "    data.append((en_tensor_, de_tensor_))\n",
        "  return data\n",
        "\n",
        "#pre-process\n",
        "train_data = data_process(train_filepaths)\n",
        "val_data = data_process(val_filepaths)\n",
        "test_data = data_process(test_filepaths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wo6Mz24Itw7B"
      },
      "outputs": [],
      "source": [
        "#NOTE: if you are low on credits or testing only use a piece of the data\n",
        "train_data = train_data[:1000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8FHO7cfuHz_",
        "outputId": "f7f52488-2367-4c97-f365-2d27104cb396"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "len(train_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyZRMk_Yi_YJ"
      },
      "source": [
        "Define the device."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XNS-Dmk0i_YJ",
        "outputId": "86a176cf-d03d-4e62-a58d-24f48deee099"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWUIbF0Wi_YK"
      },
      "source": [
        "Create the iterators."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tCHlMlZYi_YK"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 8\n",
        "PAD_IDX = de_vocab['<pad>']\n",
        "BOS_IDX = de_vocab['<bos>']\n",
        "EOS_IDX = de_vocab['<eos>']\n",
        "\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def generate_batch(data_batch):\n",
        "    en_batch, de_batch = [], []\n",
        "    for (en_item, de_item) in data_batch:\n",
        "        de_batch.append(torch.cat([torch.tensor([BOS_IDX]), de_item, torch.tensor([EOS_IDX])], dim=0))\n",
        "        en_batch.append(torch.cat([torch.tensor([BOS_IDX]), en_item, torch.tensor([EOS_IDX])], dim=0))\n",
        "    de_batch = pad_sequence(de_batch, padding_value=PAD_IDX, batch_first=True)\n",
        "    en_batch = pad_sequence(en_batch, padding_value=PAD_IDX, batch_first=True)\n",
        "    return en_batch, de_batch\n",
        "\n",
        "train_iter = DataLoader(train_data, batch_size=BATCH_SIZE,\n",
        "                        shuffle=True, collate_fn=generate_batch)\n",
        "valid_iter = DataLoader(val_data, batch_size=BATCH_SIZE,\n",
        "                        shuffle=False, collate_fn=generate_batch)\n",
        "test_iter = DataLoader(test_data, batch_size=BATCH_SIZE,\n",
        "                       shuffle=False, collate_fn=generate_batch)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjnN1Aj0i_YK"
      },
      "source": [
        "# Encoder\n",
        "\n",
        "single layer bidirectional GRU, however we now use a *bidirectional RNN*.\n",
        "\n",
        "\n",
        "\n",
        "$$\\begin{align*}\n",
        "h_t^\\rightarrow &= \\text{EncoderGRU}^\\rightarrow(e(x_t^\\rightarrow),h_{t-1}^\\rightarrow)\\\\\n",
        "h_t^\\leftarrow &= \\text{EncoderGRU}^\\leftarrow(e(x_t^\\leftarrow),h_{t-1}^\\leftarrow)\n",
        "\\end{align*}$$\n",
        "\n",
        "\n",
        "The GRU returns `outputs` and `hidden`.\n",
        "\n",
        "`outputs`  size **[batch size, srclen, H * num directions]**\n",
        "\n",
        "\n",
        "`hidden` size **[n layers * num directions, batch size, hid dim]**\n",
        "\n",
        " **[-2, :, :]**  top layer forward RNN hidden state after the final time-step\n",
        "\n",
        " **[-1, :, :]** top layer backward RNN hidden state after the final time-step\n",
        "\n",
        "The decoder needs a single context vector (`hidden`) $z$,  as the initial hidden state,\n",
        "\n",
        "$$z=\\tanh(g(h_T^\\rightarrow, h_T^\\leftarrow)) = \\tanh(g(z^\\rightarrow, z^\\leftarrow)) = s_0$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zf4mEJnhi_YK"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "\n",
        "        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional=True, batch_first=True)\n",
        "\n",
        "        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src):\n",
        "\n",
        "        #[B, srclen]\n",
        "        #embedd and dropout\n",
        "        embedded = self.embedding(src)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        #[B, srclen, emb]\n",
        "        outputs, hidden = self.rnn(embedded)\n",
        "\n",
        "        #[B, srclen, H*2]\n",
        "        #h[B, n layers * num directions, hid dim]\n",
        "        #[forward_1, backward_1, forward_2, backward_2, ...]\n",
        "        #[-2, :, : ] last state forward RNN\n",
        "        #[-1, :, : ] last state backward RNN\n",
        "        hidden_forward = hidden[-2, :, :]\n",
        "        hidden_backward = hidden[-1, :, :]\n",
        "        hidden = torch.cat((hidden_forward, hidden_backward), dim=1)\n",
        "        hidden = torch.tanh(self.fc(hidden))\n",
        "\n",
        "        return outputs, hidden"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Attention\n",
        "\n",
        "## Luong Attention\n",
        "\n",
        "Takes in the previous hidden state of the decoder, $s_{t-1}$, and all of the stacked forward and backward hidden states from the encoder, $H$.\n",
        "\n",
        "The layer will output an attention vector, $a_t$, that is the length of the source sentence, each element is between 0 and 1 and the entire vector sums to 1.\n",
        "\n",
        "\n",
        "Compute the *score* between the previous decoder hidden state and the encoder hidden states. $E_t$, between them by concatenating them together and passing them through a linear layer (`attn`) and a $\\tanh$ activation function.\n",
        "\n",
        "$$E_t = \\tanh(\\text{attn}(s_{t-1}, H))$$\n",
        "\n",
        ".\n",
        "\n",
        "$$\\hat{a}_t = v E_t$$\n",
        "\n",
        "\n",
        "\n",
        "Attention vector constraints between 0 and 1 and the vector summing to 1 by passing it through a $\\text{softmax}$ .\n",
        "\n",
        "$$a_t = \\text{softmax}(\\hat{a_t})$$\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vzSF7PQpERGP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kP4jL01Vi_YL"
      },
      "outputs": [],
      "source": [
        "class LuongAttention(nn.Module):\n",
        "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\n",
        "        self.v = nn.Linear(dec_hid_dim, 1, bias = False)\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs): #keys, query\n",
        "\n",
        "        #[B, H]\n",
        "        #[B, srclen, H* 2]\n",
        "        batch_size = encoder_outputs.shape[0]\n",
        "        src_len = encoder_outputs.shape[1]\n",
        "\n",
        "        #x times decoder hidden state for src_len\n",
        "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
        "\n",
        "        #[B, srclen, H]\n",
        "        scores = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim = 2)))\n",
        "\n",
        "\n",
        "        ##[B, srclen, H]\n",
        "        attention = self.v(scores).squeeze(2)\n",
        "\n",
        "        #[B,  srclen]\n",
        "\n",
        "        return F.softmax(attention, dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sFSa__e8_ZQB"
      },
      "outputs": [],
      "source": [
        "#this thing took ages off of my life\n",
        "\n",
        "class BahdanauAttention(nn.Module):\n",
        "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.Wa = nn.Linear(dec_hid_dim, dec_hid_dim, bias=False)\n",
        "        self.Ua = nn.Linear(enc_hid_dim * 2, dec_hid_dim, bias=False)\n",
        "        self.Va = nn.Linear(dec_hid_dim, 1, bias=False)\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs): #keys, query\n",
        "\n",
        "        batch_size = encoder_outputs.shape[0]\n",
        "        src_len = encoder_outputs.shape[1]\n",
        "\n",
        "        #x times decoder hidden state for src_len\n",
        "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
        "\n",
        "        scores = torch.tanh(self.Wa(hidden) + self.Ua(encoder_outputs))\n",
        "        scores = self.Va(scores).squeeze(2)\n",
        "\n",
        "        weights = torch.softmax(scores, dim=1)\n",
        "\n",
        "        return weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWvboaMki_YL"
      },
      "source": [
        "# Decoder\n",
        "\n",
        "\n",
        "The decoder contains the attention `attention`, and we use this attention vector to create a weighted source vector, $w_t$ `weighted`, which is a weighted sum of the encoder hidden states, $H$, using $a_t$ as the weights.\n",
        "\n",
        "$$w_t = a_t H$$\n",
        "\n",
        "The embedded input word, $d(y_t)$ `embedded`, the weighted source vector, $w_t$, and the previous decoder hidden state, $s_{t-1}$, are all passed into the decoder RNN, with $d(y_t)$ and $w_t$ concatenated.\n",
        "\n",
        "$$s_t = \\text{DecoderGRU}(d(y_t), w_t, s_{t-1})$$\n",
        "\n",
        "Pass $d(y_t)$, $w_t$ and $s_t$ through the linear layer, $f$, to make a prediction of the next word in the target sentence, $\\hat{y}_{t+1}$ `prediciton`.\n",
        "\n",
        "$$\\hat{y}_{t+1} = f(d(y_t), w_t, s_t)$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t_J_RZPbi_YL"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):\n",
        "        super().__init__()\n",
        "\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        self.attention = attention\n",
        "\n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "\n",
        "        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim, batch_first=True)\n",
        "\n",
        "        self.fc_out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + emb_dim, output_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        #attention\n",
        "        #self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\n",
        "        #self.v = nn.Linear(dec_hid_dim, 1, bias = False)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "\n",
        "        #[B]\n",
        "        #[B, H]\n",
        "        #[B, srclen, H * 2]\n",
        "        #unsqueeze https://pytorch.org/docs/stable/generated/torch.unsqueeze.html\n",
        "        input = input.unsqueeze(1)\n",
        "\n",
        "        #[B, 1]\n",
        "        #embeded and dropout\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "\n",
        "        #[B, 1, emb]\n",
        "        #call attention\n",
        "        a = self.attention(hidden, encoder_outputs)\n",
        "\n",
        "        #[B, srclen]\n",
        "        a = a.unsqueeze(1)\n",
        "\n",
        "        #encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "\n",
        "        #[B, srclen, H * 2]\n",
        "        #w with bmm between (a, embedded)\n",
        "        #https://pytorch.org/docs/stable/generated/torch.bmm.html\n",
        "        weighted =  torch.bmm(a, encoder_outputs) #weighted = a.bmm(encoder_outputs)\n",
        "\n",
        "        #[B, 1, H * 2]\n",
        "        # permute\n",
        "        #weighted = weighted.permute(1, 0, 2)\n",
        "\n",
        "        #[1, B, H * 2]\n",
        "        #concat embedded and weighted\n",
        "        rnn_input = torch.cat((embedded, weighted), dim=2)\n",
        "\n",
        "        #[1, B, (H * 2) + emb]\n",
        "        #[B, 1, (H * 2) + emb]\n",
        "        #permute https://pytorch.org/docs/stable/generated/torch.permute.html\n",
        "       # rnn_input = rnn_input.permute(1, 0, 2)\n",
        "        #unsqueeze\n",
        "        #hidden = hidden.unsqueeze(0)\n",
        "\n",
        "        #call rnn with (rnn_input, hidden)\n",
        "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
        "\n",
        "        #[B, M, H * n directions]\n",
        "        #[B, n layers * n directions, H]\n",
        "        #[1, B, H]\n",
        "        # drop specific dims\n",
        "        embedded = embedded.squeeze(1)\n",
        "        output = output.squeeze(1)\n",
        "        weighted = weighted.squeeze(1)\n",
        "\n",
        "        #prediction layer with concatenated outputm weighted and embedded\n",
        "        prediction = self.fc_out(torch.cat((output, weighted, embedded), dim=1))\n",
        "\n",
        "        #[B, V]\n",
        "\n",
        "        return prediction, hidden.squeeze(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d33lfUX9i_YL"
      },
      "source": [
        "# Seq2Seq\n",
        "\n",
        "contains encoder, and decoder\n",
        "\n",
        "steps:\n",
        "- the `outputs` is created to hold all predictions, $\\hat{Y}$\n",
        "- the source sequence, $X$, is fed into the encoder to receive $z$ and $H$\n",
        "- the initial decoder hidden state is set to be the `context` vector, $s_0 = z = h_T$\n",
        "- batch of `<sos>` tokens as the first `input`, $y_1$\n",
        "- decode within a loop:\n",
        "  - inserting the input token $y_t$, previous hidden state, $s_{t-1}$, and all encoder outputs, $H$, into the decoder\n",
        "  - prediction, $\\hat{y}_{t+1}$, and a new hidden state, $s_t$\n",
        "  - decide if we are going to teacher force or not, setting the next input as appropriate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GwoMHRFxi_YL"
      },
      "outputs": [],
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "\n",
        "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
        "\n",
        "        #[B, srclen]\n",
        "        #[B, trglen]\n",
        "        #teacher_forcing_ratio is probability to use teacher forcing\n",
        "        # 0.75 teacher forcing 75% of the time\n",
        "\n",
        "        batch_size = trg.shape[0]\n",
        "        trg_len = trg.shape[1]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "\n",
        "        #torch zeros [B, trglen, trgV]\n",
        "        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)\n",
        "\n",
        "        #encoder_outputs is all hidden states of the input sequence\n",
        "        #hidden is the final forward and backward hidden states, passed through a linear layer\n",
        "        #call encoder\n",
        "        encoder_outputs, hidden = self.encoder(src)\n",
        "\n",
        "        #first input to the decoder is the <sos> tokens\n",
        "        input = trg[:,0]\n",
        "        # unroll RNN\n",
        "        for t in range(1, trg_len):\n",
        "\n",
        "            #insert input token embedding, previous hidden state and all encoder hidden states\n",
        "            #call decoder\n",
        "            output, hidden = self.decoder(input, hidden, encoder_outputs)\n",
        "\n",
        "            #predictions\n",
        "            #save hidden\n",
        "            outputs[:, t] = output\n",
        "\n",
        "            #teacher forcing\n",
        "            # if random num < teacher ratio\n",
        "            teacher_force =  random.random() < teacher_forcing_ratio\n",
        "\n",
        "            #greedy search\n",
        "            #argmax over predictions\n",
        "            #https://pytorch.org/docs/stable/generated/torch.argmax.html#torch.argmax\n",
        "            top1 = output.argmax(1)\n",
        "\n",
        "            #if teacher forcing, use gold token as next input\n",
        "            #if not, use predicted token\n",
        "            input = trg[:, t] if teacher_force else top1\n",
        "\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e06yvLvei_YL"
      },
      "source": [
        "## Training the Seq2Seq Model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5jC9nxari_YM"
      },
      "outputs": [],
      "source": [
        "INPUT_DIM = len(en_vocab) #TODO #DID\n",
        "OUTPUT_DIM = len(de_vocab)\n",
        "ENC_EMB_DIM = 256\n",
        "DEC_EMB_DIM = 256\n",
        "ENC_HID_DIM = 512\n",
        "DEC_HID_DIM = 512\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.2\n",
        "\n",
        "#attn = LuongAttention(ENC_HID_DIM, DEC_HID_DIM)\n",
        "#TODO!!\n",
        "attn = BahdanauAttention(ENC_HID_DIM, DEC_HID_DIM)\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
        "\n",
        "model = Seq2Seq(enc, dec, device).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DGLEjaJu2NjY",
        "outputId": "e437b4eb-8d1f-4b95-85d6-b7412a42afd0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "17949\n",
            "19168\n"
          ]
        }
      ],
      "source": [
        "print(len(en_vocab)) #BPE size 16k approx\n",
        "print(len(de_vocab))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rUD45Ys5i_YM",
        "outputId": "f298dc10-8bf5-47a7-b2ae-731fcc2eab27"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(17949, 256)\n",
              "    (rnn): GRU(256, 512, batch_first=True, bidirectional=True)\n",
              "    (fc): Linear(in_features=1024, out_features=512, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (attention): BahdanauAttention(\n",
              "      (Wa): Linear(in_features=512, out_features=512, bias=False)\n",
              "      (Ua): Linear(in_features=1024, out_features=512, bias=False)\n",
              "      (Va): Linear(in_features=512, out_features=1, bias=False)\n",
              "    )\n",
              "    (embedding): Embedding(19168, 256)\n",
              "    (rnn): GRU(1280, 512, batch_first=True)\n",
              "    (fc_out): Linear(in_features=1792, out_features=19168, bias=True)\n",
              "    (dropout): Dropout(p=0.2, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        if 'weight' in name:\n",
        "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
        "        else:\n",
        "            nn.init.constant_(param.data, 0)\n",
        "\n",
        "model.apply(init_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bmY0twjqi_YM",
        "outputId": "15982034-5cc7-43f3-978c-b84fcc77f2be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 50,302,944 trainable parameters\n"
          ]
        }
      ],
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6M5jMx9i_YM"
      },
      "source": [
        "We create an optimizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GkFpvqEii_YM"
      },
      "outputs": [],
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POJ6uKGei_YM"
      },
      "source": [
        "We initialize the loss function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "McD_2eGsi_YM"
      },
      "outputs": [],
      "source": [
        "TRG_PAD_IDX = de_vocab['<pad>'] #TODO\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBn2uizpi_YM",
        "outputId": "9f6d060d-fc41-47ec-f6e1-35abe15318ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        }
      ],
      "source": [
        "print(TRG_PAD_IDX)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZWBC8pDAi_YN"
      },
      "outputs": [],
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for (src, trg) in tqdm(iterator):\n",
        "\n",
        "        src, trg = src.to(device), trg.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model(src, trg)\n",
        "\n",
        "        #[B, trg len]\n",
        "        #[B, trg len, H]\n",
        "\n",
        "        output = output.permute(1, 0, 2)\n",
        "\n",
        "        output_dim = output.shape[-1]\n",
        "        trg = trg.permute(1, 0)\n",
        "\n",
        "\n",
        "        #[B * (trg len - 1), output dim]\n",
        "        #[B * (trg len - 1)]\n",
        "        output = output[1:].reshape(-1, output_dim)\n",
        "        trg = trg[1:].reshape(-1)\n",
        "\n",
        "        loss = criterion(output, trg)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lJGxS6mFi_YN"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    epoch_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for (src, trg) in iterator:\n",
        "            src, trg = src.to(device), trg.to(device)\n",
        "\n",
        "            output = model(src, trg, 0) #turn off teacher forcing\n",
        "\n",
        "            #[B * (trg len - 1), output dim]\n",
        "            #[B * (trg len - 1)]\n",
        "            output = output.permute(1, 0, 2)\n",
        "            output_dim = output.shape[-1]\n",
        "            trg = trg.permute(1, 0)\n",
        "            output = output[1:].reshape(-1, output_dim)\n",
        "            trg = trg[1:].reshape(-1)\n",
        "\n",
        "            loss = criterion(output, trg)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AFGhRjIMi_YN",
        "outputId": "b2286a04-c784-4038-900b-8f4610b805ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 13/13 [00:05<00:00,  2.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01\n",
            "\tTrain Loss: 9.321\tTrain PPL: 11167.060\n",
            "\t Validation Loss: 8.799\tValidation PPL: 6627.976\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 13/13 [00:05<00:00,  2.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 02\n",
            "\tTrain Loss: 7.600\tTrain PPL: 1999.181\n",
            "\t Validation Loss: 9.166\tValidation PPL: 9570.115\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 13/13 [00:05<00:00,  2.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 03\n",
            "\tTrain Loss: 7.205\tTrain PPL: 1346.577\n",
            "\t Validation Loss: 9.860\tValidation PPL: 19155.350\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 13/13 [00:05<00:00,  2.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 04\n",
            "\tTrain Loss: 7.014\tTrain PPL: 1112.299\n",
            "\t Validation Loss: 9.891\tValidation PPL: 19758.134\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 13/13 [00:06<00:00,  2.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 05\n",
            "\tTrain Loss: 6.852\tTrain PPL: 945.661\n",
            "\t Validation Loss: 10.307\tValidation PPL: 29927.322\n"
          ]
        }
      ],
      "source": [
        "N_EPOCHS = 5\n",
        "CLIP = 1\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    train_loss = train(model, train_iter, optimizer, criterion, CLIP)\n",
        "    valid_loss = evaluate(model, valid_iter, criterion)\n",
        "\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'model.pt')\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02}')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f}\\tTrain PPL: {np.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Validation Loss: {valid_loss:.3f}\\tValidation PPL: {np.exp(valid_loss):7.3f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2IiI41XDs4bj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "meHcb9oOi_YN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f010bd6b-0e07-47e3-be0e-8eba64e0d7a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTest Loss: 8.796\tTest PPL: 6609.470\n"
          ]
        }
      ],
      "source": [
        "#load model from file\n",
        "model.load_state_dict(torch.load('model.pt'))\n",
        "\n",
        "test_loss = evaluate(model, test_iter, criterion)\n",
        "\n",
        "print(f'\\tTest Loss: {test_loss:.3f}\\tTest PPL: {np.exp(test_loss):7.3f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g7dql_q_i_YN"
      },
      "outputs": [],
      "source": [
        "#clean mem\n",
        "del model\n",
        "del train_iter\n",
        "del valid_iter\n",
        "del test_iter\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z78J4LTji_YO"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}